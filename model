import torch
import torch . nn as nn
import torch
import torch . nn as nn
from torch.utils.data import DataLoader , Dataset,TensorDataset
from torch.autograd import Variable
import numpy as np
import pandas as pd


def load_vocab ():
    vocab = open('./vocab.txt', encoding='utf -8').readlines()
    slice2idx={}
    idx2slice={}
    cnt = 0
    for char in vocab:
        char = char.strip('\n')
        slice2idx[char]=cnt
        idx2slice[cnt]=char
        cnt += 1
        return slice2idx,idx2slice


def padding(text, maxlen=800):
    pad_text = []
    for sentence in text:
        pad_sentence = np.zeros(maxlen).astype('int64')
        cnt = 0
        for index in sentence:
            pad_sentence[cnt] = index
            cnt += 1
        if cnt == maxlen:
            break
    pad_text.append(pad_sentence.tolist())
    return pad_text


def char_index(text_a, text_b):
    slice2idx,idx2slice=load_vocab()
    a_list, b_list=[],[]

    for a_sentence, b_sentence in zip(text_a, text_b):
        a, b = [], []

    for slice in lst_gram ( a_sentence ):

        if slice in slice2idx.keys():
            a.append(slice2idx[slice])
        else:
            a.append(1)

    for slice in lst_gram ( b_sentence ):
        if slice in slice2idx.keys():
            b.append(slice2idx[slice])
        else:
            b.append(1)

    a_list.append(a)
    b_list.append(b)

    a_list=padding(a_list)
    b_list=padding(b_list)

    return a_list,b_list

def load_char_data(filename):
    df = pd.read_table(filename, sep='\t',error_bad_lines=False)
    text_a = df['#1 String'].values
    text_b = df['#2 String'].values
    label = df['Quality'].values

    a_index, b_index = char_index(text_a, text_b)
    return np.array(a_index), np.array(b_index), np.array(label)

def n_gram( word , n=3) :
    s = []
    word = '#' + word + '#'
    for i in range(len(word) -2):
        s.append(word[i: i + 3])
        return s


def lst_gram(lst, n=3):
    s = []
    for word in str(lst).lower ().split ():
        s.extend(n_gram(word))
        return s


        vocab = []
        file_path='D:/1\Desktop/Nn/MRPC'
        files = ['msr_paraphrase_train.txt', 'msr_paraphrase_test.txt']

        for file in files:
            f = open(file_path + file,encoding='utf_8').readlines()
            for i in range(1, len(f)):  #从遍历因为下标为表头
                # (10 gold_label sentencel ) sentence2
                s1, s2 = f[i][2:].strip('\n').split('\t')
                vocab.extend(lst_gram(s1))
                vocab.extend(lst_gram(s2))

                vocab = set(vocab)
                vocab_list = ['[PAD]', '[UNK]']
                vocab_list.extend(list(vocab))

                vocab_file ='./vocab.txt'
                with open(vocab_file, 'w', encoding='utf -8') as f:
                    for slice in vocab_list:
                        f.write(slice)
                        f.write('\n')




class DSSM(nn.Module):
    def __init__(self ):
        super(DSSM, self ).__init__()
        self.embedding = nn.Embedding(10041,300)
        self.linear1=nn.Linear(300,256)
        self.linear2=nn.Linear(256,128)
        self.linear3 = nn.Linear(128,64)
        self.dropout=nn.Dropout(0.2)

    def forward(self, a, b):
            a = self.embedding(a).sum(1)
            b = self.embedding(b).sum(1)

            a = torch.tanh(self.linear1(a))
            a = self.dropout(a)
            a = torch.tanh(self.linear2(a))
            a = self.dropout(a)
            a = torch.tanh(self.linear3(a))
            a = self.dropout(a)

            b = torch.tanh(self.linear1(b))
            b = self.dropout(b)
            b = torch.tanh(self.linear2(b))
            b = self.dropout(b)
            b = torch.tanh(self.linear3(b))
            b = self.dropout(b)


            cosine = torch.cosine_similarity(a,b,dim=1,eps=1e-8)
            return cosine

    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                torch.nn.init.xavier_uniform_(m.weight, gain=1)









class MRPCDataset( Dataset ):
    def  __init__(self,filepath):
        self.path = filepath
        self.a_index,self.b_index,self.label=load_char_data(filepath)

    def __len__(self):
        return len(self.a_index)

    def __getitem__(self,idx):
        return  self.a_index[idx],self.b_index[idx],self.label[idx]


if __name__ == '__main__':
    train_data = MRPCDataset(r"C:\Users\华硕\PycharmProjects\pythonProject\venv\nn_bylw_fei\MRPC\msr_paraphrase_train.txt")
    test_data = MRPCDataset(r"C:\Users\华硕\PycharmProjects\pythonProject\venv\nn_bylw_fei\MRPC\msr_paraphrase_test.txt")
    train_loader = DataLoader(dataset=train_data, batch_size=50, shuffle = True )
    test_loader = DataLoader(dataset=test_data, batch_size=50, shuffle=True)

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    dssm = DSSM().to(device)
    dssm._initialize_weights()
    optimizer = torch.optim.Adam(dssm.parameters(), lr = 0.0005)
    loss_func = nn.CrossEntropyLoss()


    for epoch in range(100):
        total=0
        correct=0
        for step, (text_a, text_b, tst_l) in enumerate(train_loader):
            a = Variable(text_a.to(device).long())
            b = Variable(text_b.to(device).long())
            l = Variable(torch.LongTensor(tst_l).to(device))
            pos_res = dssm(a, b)
            neg_res = 1-pos_res
            out = torch.stack([neg_res, pos_res], 1).to(device)

            loss = loss_func(out, l)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        # if(step + 1) % 20 == 0:
        #     total = 0
        #     correct = 0
        for (test_a, test_b, test_l) in test_loader:

            tst_a = Variable(test_a.to(device).long())
            tst_b = Variable(test_b.to(device).long())
            tst_l = Variable(torch.LongTensor(test_l).to(device))
            pos_res = dssm(tst_a, tst_b)
            neg_res = 1-pos_res
            out = torch.max(torch.stack([neg_res, pos_res], 1).to(device), 1)[1]
        if out.size()==tst_l.size():
            total += tst_l.size(0)
            correct += (out == test_l).sum().item()
            print('[Epoch]:', epoch + 1, '训练loss:', loss.item())
            print('[Epoch]:', epoch + 1, '测试集准确率: ',(correct*1.0 / total))
            torch.save(dssm, './dssm_pr.pkl')
